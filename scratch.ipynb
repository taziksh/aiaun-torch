{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be377f9-9eec-4983-bb3f-572bcf169939",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4d4052-1d3d-417f-b2e3-4295a37dc9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.get_default_dtype())\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1ae06e-aacf-48d9-bf2a-3c5bce2a463e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "#NOTE: they used train split, wiht ~3.5M examples\n",
    "ds = load_dataset(\"wmt/wmt14\", \"de-en\", split='train')\n",
    "training_data = ds['translation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20798bd-cc5f-4e2c-861f-e336cb8f0c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "import os\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "num_layers = 6\n",
    "seq_len = 1024 # aka n_positions \n",
    "d_model = 512\n",
    "\n",
    "class FFN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()            \n",
    "        \n",
    "        self.d_ff = d_model*4\n",
    "        self.W_1 = torch.nn.Linear(d_model, self.d_ff)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.W_2 = torch.nn.Linear(self.d_ff, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # assert x.shape == (seq_len, d_model)    \n",
    "        out = self.W_1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.W_2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class MHA(torch.nn.Module):\n",
    "    def __init__(self, h=8, has_mask=False):  \n",
    "        super().__init__()          \n",
    "        \n",
    "        self.has_mask = has_mask\n",
    "        self.d_k = d_model // h\n",
    "\n",
    "        self.d_v = self.d_k        \n",
    "        \n",
    "        self.scale = 1/math.sqrt(self.d_k)\n",
    "\n",
    "        assert d_model == 512\n",
    "        assert self.d_k == 64\n",
    "        assert self.d_v == self.d_k\n",
    "\n",
    "        self.W_Q = torch.nn.Parameter(torch.nn.init.xavier_uniform_(torch.zeros(size=(d_model, self.d_k))))\n",
    "        self.W_K = torch.nn.Parameter(torch.nn.init.xavier_uniform_(torch.zeros(size=(d_model, self.d_k))))\n",
    "        self.W_V = torch.nn.Parameter(torch.nn.init.xavier_uniform_(torch.zeros(size=(d_model, self.d_v))))\n",
    "        self.W_O = torch.nn.Parameter(torch.nn.init.xavier_uniform_(torch.zeros(size=(h*self.d_v, d_model))))\n",
    "                \n",
    "    #TODO: why separate x_k and x_v\n",
    "    def attn(self, x_k, x_v):\n",
    "        Q = x_k @ self.W_Q\n",
    "        K = x_k @ self.W_K\n",
    "        V = x_v @ self.W_V\n",
    "\n",
    "        mask = torch.ones(seq_len, seq_len).to(device) \n",
    "        if self.has_mask:\n",
    "            mask = torch.tril(mask)\n",
    "        \n",
    "        # softmax along dim=0 of Q@K.T => seq_len\n",
    "        sm = torch.softmax(input=self.scale*mask*(Q@K.T), dim=0)\n",
    "        head = sm @ V\n",
    "        return head\n",
    "\n",
    "    def forward(self, x_k, x_v):\n",
    "        assert x_k.shape == (seq_len, d_model)\n",
    "        assert x_v.shape == (seq_len, d_model)\n",
    "        \n",
    "        heads = torch.cat([self.attn(x_k, x_v) for _ in range(8)], dim=1)\n",
    "        res = heads @ self.W_O\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8259151c-8567-4f1d-b0c9-05754c2a19bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dim = 50257+1\n",
    "\n",
    "class Embed(torch.nn.Module):\n",
    "    def __init__(self, vocab_dim, d_model):\n",
    "        super().__init__()\n",
    "        self.emb = torch.nn.Embedding(num_embeddings=vocab_dim, embedding_dim=d_model, padding_idx=50257)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.emb(x)\n",
    "        return res\n",
    "\n",
    "class PosEmbed():\n",
    "    def __init__(self):\n",
    "        self.emb = torch.zeros(seq_len, d_model).to(device)\n",
    "\n",
    "        for pos in range(seq_len):\n",
    "            for i in range(d_model//2):\n",
    "                arg = pos/(10000**(2*i/d_model))\n",
    "                if i % 2 == 0:\n",
    "                    self.emb[pos, 2*i] = math.sin(arg)\n",
    "                else:\n",
    "                    self.emb[pos, 2*i+1] = math.cos(arg)\n",
    "\n",
    "    def fwd(self):   \n",
    "        return self.emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a12b85-c7c8-4d17-b61c-6d7ef8fa402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.M = MHA(has_mask=False)\n",
    "        self.F = FFN()\n",
    "        self.dropout = torch.nn.Dropout(p=0.1)\n",
    "        self.LN = torch.nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mha = self.M(x_k=x, x_v=x)\n",
    "        mha = self.dropout(mha)\n",
    "\n",
    "        sl1 = self.LN(x+mha)\n",
    "\n",
    "        ffn = self.F(sl1)\n",
    "        ffn = self.dropout(ffn)\n",
    "\n",
    "        sl2 = self.LN(sl1 + ffn)\n",
    "\n",
    "        return sl2\n",
    "\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.M1 = MHA(has_mask=True)\n",
    "        self.M2 = MHA(has_mask=False)\n",
    "        self.F = FFN()\n",
    "        self.dropout = torch.nn.Dropout(p=0.1)\n",
    "        self.LN = torch.nn.LayerNorm(d_model)        \n",
    "    \n",
    "    def forward(self, x, enc_out):\n",
    "        mmha = self.M1(x_k=x, x_v=x)\n",
    "        mmha = self.dropout(mmha)\n",
    "\n",
    "        sl1 = self.LN(x + mmha)\n",
    "\n",
    "        mha = self.M2(x_k=enc_out, x_v=sl1)\n",
    "        mha = self.dropout(mha)\n",
    "        \n",
    "        sl2 = self.LN(sl1 + mha)\n",
    "\n",
    "        ffn = self.F(sl2)\n",
    "        ffn = self.dropout(ffn)\n",
    "\n",
    "        sl3 = self.LN(sl2 + ffn)\n",
    "\n",
    "        return sl3\n",
    "\n",
    "class EncoderDecoder(torch.nn.Module):      \n",
    "    def __init__(self):\n",
    "        super().__init__()        \n",
    "        self.encs = torch.nn.ModuleList([Encoder() for _ in range(num_layers)])\n",
    "        self.decs = torch.nn.ModuleList([Decoder() for _ in range(num_layers)])\n",
    "        self.pos_embed = PosEmbed()\n",
    "        self.dropout = torch.nn.Dropout(p=0.1)        \n",
    "\n",
    "    def forward(self, x):\n",
    "        enc_out = x\n",
    "        for enc in self.encs:\n",
    "            enc_out = enc(enc_out)\n",
    "\n",
    "        dec_out = enc_out + self.pos_embed.fwd()\n",
    "        dec_out = self.dropout(dec_out)        \n",
    "        \n",
    "        for dec in self.decs:\n",
    "            dec_out = dec(enc_out, dec_out)\n",
    "            \n",
    "        return dec_out   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af61bda-d2c8-407c-97bf-ebfa23d6fc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "       super().__init__()\n",
    "       self.input_embed = Embed(vocab_dim=vocab_dim, d_model=d_model)\n",
    "       self.enc_dec = EncoderDecoder()       \n",
    "       self.linear = torch.nn.Linear(d_model, vocab_dim) # (seq_len, d_model) -> (seq_len, vocab_dim)\n",
    "       self.pos_embed = PosEmbed()\n",
    "       self.dropout = torch.nn.Dropout(p=0.1) \n",
    "        \n",
    "    def forward(self, x):\n",
    "       #Nit: torch.nn.Sequential\n",
    "        \n",
    "       #TODO: fix this assert\n",
    "       print(x.shape) \n",
    "       # assert x.shape == (seq_len)\n",
    "        \n",
    "       out = self.input_embed(x) + self.pos_embed.fwd()\n",
    "       assert out.shape == (seq_len, d_model) # embedding  \n",
    "\n",
    "        \n",
    "       out = self.dropout(out)\n",
    "        \n",
    "       out = self.enc_dec(out)     \n",
    "       out = self.linear(out)\n",
    "\n",
    "       assert out.shape == (seq_len, vocab_dim) \n",
    "\n",
    "       # Softmax over dim=0, seq_len\n",
    "       softmax = torch.softmax(input=out, dim=0) \n",
    "       return softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da610fa-3b66-48c2-a381-f627d1179cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = Transformer().to(device)\n",
    "\n",
    "#Nit: tokenizer fork warning\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "num_epochs = 10\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "losses = []\n",
    "\n",
    "#TODO: scheduler\n",
    "# warmup_steps = 4000\n",
    "# step_num = 1\n",
    "# lr = d_model**(-0.5)*min(step_num**(-0.5), step_num * warmup_steps**(-1.5))\n",
    "optim = torch.optim.Adam(params=model.parameters(), betas=(0.9,0.98), eps=10E-9)#, lr=lr)\n",
    "\n",
    "for epoch in range(num_epochs+1):\n",
    "    for sample in training_data[:2]:\n",
    "        optim.zero_grad()\n",
    "\n",
    "        tokenized_de = tokenizer.encode(sample['de'], padding='max_length', max_length=seq_len)\n",
    "        tokenized_en = tokenizer.encode(sample['en'], padding='max_length', max_length=seq_len)\n",
    "        \n",
    "        targets = torch.tensor(tokenized_en).to(device)\n",
    "        inputs = torch.tensor(tokenized_de).to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        losses.append(loss)\n",
    "\n",
    "        loss.backward()        \n",
    "        optim.step()\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ece7ed-2963-44bc-9f8d-7ab3c1d83ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
